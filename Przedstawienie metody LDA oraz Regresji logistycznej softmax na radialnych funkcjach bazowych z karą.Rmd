---
title: "LDA-method-and-Softmax-Ridge-Regression-with-Radial-Based-Functions-in-R"
author: "Miko≈Çaj Ozimek"
date: "2024-02-08"
output: github_document
---
```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Pakiety -----------------------------------------------------------------
library(MASS)
library(klaR)
library(caret)
library(ggplot2)
library(dplyr)
library(glmnet)
library(nnet)
library(lattice)
library(stepPlr)
```

<font size = 5>Initial data check.</font>

<font size = 4>Let's check if there are any non-numeric or unknown values in the data.</font>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
dane <- read.csv("dane34.csv")

sprawdz_dane <- function(x){
  bledy = 0
  for(zmienna in colnames(x)){
    if(TRUE %in% cbind(is.na(x$zmienna),is.nan(x$zmienna))){
      bledy = bledy + 1
    }
  }
print(bledy)
}
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
sprawdz_dane2 <- function(x){
  odstajace = 0
  for(zmienna in colnames(x)){
    for(i in x$zmienna){
      if(i > 1.5*IQR(x$zmienna)){
        odstajace = odstajace + 1
      }
    }
  }
print(odstajace)
}
```

<font size = 4>We do not need to correct the data (No imputation required). </font>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
sprawdz_dane(dane)
```

<font size = 4>Let's see if the data requires scaling (if there are any outliers).</font>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
sprawdz_dane2(dane)
```

<font size = 4>Let's see what the types of individual variables are.</font>


```{r, echo=FALSE, warning=FALSE, message=FALSE}
cbind(A=class(dane$A),B=class(dane$B),Class=class(dane$Class))
dane$Class <- factor(dane$Class)
class(dane$Class)
```
<font size = 4>Only the type of the dependent variable had to be changed to factor.</font>

<font size = 5>Classification using the LDA algorithm.</font>

<font size = 4>We will see a preliminary data graph. </font>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
ggplot(data=dane,
       mapping = aes(A,B,col=Class))+
  geom_point()
```

<font size = 4>The decision variable takes 3 values, which, as can be seen in the chart, are not linearly separable. Let's see if we can separate the observations using two canonical variables.</font>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
model_lda <- lda(Class~A+B,data = dane)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
lda_predykcje <- predict(model_lda)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
zm_kan1 <- lda_predykcje$x[,1]
zm_kan2 <- lda_predykcje$x[,2]
```

<font size = 4>Let's create a new data frame to graph the new variables</font>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
dane_kanoniczne <- data.frame(zm_kan1,zm_kan2,Class = dane$Class)
dane_kanoniczne$Class <- factor(dane_kanoniczne$Class)
head(dane_kanoniczne,4)
```

<font size = 4>In the LDA method, we project our observations relative to canonical vectors in order to increase the distance between the centers of individual classes and reduce the variance within classes. If the data is better separated from each other, the classifier will perform better, making fewer errors..</font>

<font size = 4>Let's see the classification graph after running the algorithm</font>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
partimat(Class~zm_kan1+zm_kan2,data=dane_kanoniczne)
```

<font size = 4>Points on the graph are not linearly separable. Projecting observations to a new space did not help. This is because the canonical variables are a linear combination of the original variables and not a linear function of them. We are talking here in particular about points from class 3, which are surrounded by points from class 2, which cannot be separated by executing only an algorithm.</font>

<font size = 4>Let's try to transform the observations from class 3 in order to increase the separability of the input data.</font>

```{r, echo=FALSE, warning=FALSE, message=FALSE}

trans <- dane %>%
  select(A,B,Class) %>%
  filter(Class == 3)

trans$A <- (trans$A)*1.5
trans$B <- (trans$B)*1.5

dane2 <- dane %>%
  select(A,B,Class) %>%
  filter(Class != 3)

dane_trans <- rbind(dane2,trans)
```

<font size = 4>Any transformations such as square root, logarithm or power did not give the intended results.</font>

<font size = 4>Let's see what the observations from variable 3 look like after multiplying by 1.5.</font>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
plot(dane_trans$A,dane_trans$B,col=dane_trans$Class)
```

<font size = 4>Let's create the model again. Now we will apply the transformed data.</font>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
model_lda2 <- lda(Class~A+B,data = dane_trans)
lda_trans_predykcje <- predict(model_lda2)
zm_kan_trans1 <- lda_trans_predykcje$x[,1]
zm_kan_trans2 <- lda_trans_predykcje$x[,2]
dane_trans_kanoniczne <- data.frame(zm_kan_trans1,zm_kan_trans2,Class = dane$Class)
dane_trans_kanoniczne$Class <- factor(dane_trans_kanoniczne$Class)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
partimat(Class~zm_kan_trans1+zm_kan_trans2,data = dane_trans_kanoniczne)
```

<font size = 4>Let's check how the model deals with new data. For this purpose, I will use n-fold and 10-fold cross-validation.</font>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
train(Class~zm_kan_trans1+zm_kan_trans2,data=dane_trans_kanoniczne,method="lda",
               trControl = trainControl(method="LOOCV"))
```

<font size = 4>Both the Accuracy Coefficient and the Kappa statistic were 1. This means that the classifier did not make a mistake even once.</font>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
train(Class~zm_kan_trans1+zm_kan_trans2,data = dane_trans_kanoniczne,method = "lda",
              trControl = trainControl(method="cv",number=10))
```

<font size = 4>In the case of 10-fold cross-validation, we also obtained perfect classifiers..</font>

<font size = 4>Because the input data set was not linearly separable both at the beginning and after conversion to canonical variables, some transformation of the observations from class 3 was required. If we wanted to classify a new observation with an unknown value of the target variable, having the characteristics of observations from class 3 before the transformation, then we should also transform it for correct classification. Let me illustrate this with an example.</font>

<font size = 4>Let's assume that the new observation has the given values of the independent variables: 0.010693628 and 0.02116404. Let's mark it on the chart.</font>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
plot(dane_trans$A,dane_trans$B,col=dane_trans$Class)
points(0.010693628,0.02116404,col="darkgreen")
```

<font size = 4>As you can see, it would be classified as class 2. After transformation 1.5, it should be in the correct class.</font>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
plot(dane_trans$A,dane_trans$B,col=dane_trans$Class)
points(0.010693628*1.5,0.02116404*1.5,col="darkgreen")
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
plot(dane_trans_kanoniczne$zm_kan_trans1,dane_trans_kanoniczne$zm_kan_trans2,col=dane_kanoniczne$Class)
```

<font size = 4>Observation classification chart for canonical variables.</font>


<font size = 5>Logistic regression model</font>

<font size = 4>Let's do a simple logistic regression first.</font>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
model_reglog <- multinom(Class~A+B,data= dane_trans)
predykcje <- predict(model_reglog)
table(dane_trans$Class,predykcje)
```

<font size = 4>As in the case of LDA, in order to create decision areas, observations must be linearly separable.
For this reason, I used transformed data. This is because of the decision boundary in the regression
logistic there is a certain linear function. In this case, we are dealing with softmax regression, with 3 values of the decision variable, so there will be 2 linear functions as decision functions.</font>

<font size = 4>Let's run a penalized logistic regression model.</font>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
glmnet1=glmnet(x=dane_trans[,1:2],y=dane_trans$Class,family="multinomial",
       lambda=0,alpha=0)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
model_pred1 <- train(Class~A+B,data=dane_trans,method="glmnet",
      trControl=trainControl("cv"),
      tuneGrid=data.frame(alpha=0,
      lambda=c(1,0.1,0.01,0.001,0.0001,0.00001)))
dane_pred1 <- predict.train(model_pred1)
model_pred1
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
model_pred2 <- train(Class~A+B,data=dane_trans,method="glmnet",
      trControl=trainControl("LOOCV"),
      tuneGrid=data.frame(alpha=0,
      lambda=c(1,0.1,0.01,0.001,0.0001,0.00001)))
dane_pred2 <- predict.train(model_pred2)
model_pred2
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
plot(dane_trans$A,dane_trans$B,col=dane_pred2)
```

<font size = 4>In the penalized logistic regression model, the model was wrong in a few cases.</font>

<font size = 4>Let us now perform a logistic regression model on the (non-linear) input data using basis functions.</font>

<font size = 4>Let's create new variables that are explanatory variables in the new model.</font>


```{r, echo=FALSE, warning=FALSE, message=FALSE}
set.seed(456)
nowe_dane <- data.frame()
phi1=function(punkt,sr){
  sqrt(sum((punkt-sr)^2))
}
n1=6
n2=1
mu1=seq(-7,7,length.out = n1) 
mu2=seq(-1,7,length.out = n2) 
siatka=expand.grid(mu1=mu1,mu2=mu2)
for(i in 1:(n1*n2)){
  for(j in 1:266){
    nowe_dane[j,i]=phi1(dane[j,1:2],siatka[i,])
  }
}
head(nowe_dane,4)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', fig.show='hold'}
set.seed(456)
nowa_ramka <- cbind(nowe_dane,Class = dane$Class)
model_reggrz <- glmnet(nowa_ramka[,1:6],nowa_ramka$Class, family = "multinomial",alpha=0)
```

```{r, figures-side, fig.show="hold", out.width="50%",echo=FALSE, warning=FALSE, message=FALSE}
plot(model_reggrz,xvar="lambda",label = TRUE)
```

<font size = 4>The above plots produce many different coefficients for the model due to the lambda parameter in the ridge regression. The algorithm tested the model on many different lambdas, but placed the logarithm of them on the OX axis. Note that large lambda values cause the coefficients to approach 0. This makes the model unfit. Therefore, we try to choose a smaller lambda in order to create an appropriate model.</font>

<font size = 4>We need to choose the best lambda parameter and the most optimized coefficients for our model. For this purpose, we will use cross-validation.</font>


```{r, echo=FALSE, warning=FALSE, message=FALSE}
set.seed(456)
kroswalidacja <- cv.glmnet(as.matrix(nowa_ramka[,1:6]),nowa_ramka$Class, family = "multinomial" ,alpha=0, type.measure = "class",nfolds = 10)
plot(kroswalidacja)
kroswalidacja
```

<font size = 4>In the case of the glmnet function, the classification error was only 0.06015 for the lambda determined by the function.</font>

<font size = 4>Let's see what the model looks like for a smaller number of variables resulting from radial functions.</font>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
set.seed(456)
nowa_ramka2 <- data.frame()
phi1=function(punkt,sr){
  sqrt(sum((punkt-sr)^2))
}
n1=4
n2=1
mu1=seq(-4,4,length.out = n1) 
mu2=seq(-1,4,length.out = n2) 
siatka=expand.grid(mu1=mu1,mu2=mu2)
for(i in 1:(n1*n2)){
  for(j in 1:266){
    nowa_ramka2[j,i]=phi1(dane[j,1:2],siatka[i,])
  }
}
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
set.seed(456)
kroswalidacja2 <- cv.glmnet(as.matrix(nowa_ramka2[,1:4]),nowa_ramka$Class, family = "multinomial",alpha=0, type.measure = "class",nfolds = 10)
plot(kroswalidacja2)
kroswalidacja2
```

<font size = 4>When the number of base variables was reduced to 4, the model accuracy was again 0.06015. We can therefore conclude that the previous model was suitable.</font>

<font size = 5>Let's check the classification for another base function</font>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
set.seed(456)
nowa_ramka3 = data.frame()
phi2=function(punkt,sr,os){
  exp(-sum((punkt-sr)^2)/(2*os^2))
}
n3=6
n4=1
l=1
mu3=seq(-7,7,length.out = n3)
mu4=seq(-1,7,length.out = n4)
siatka=expand.grid(mu3=mu3,mu4=mu4)
for(i in 1:(n3*n4)){
  for(j in 1:266){
    nowa_ramka3[j,i]=phi2(dane[j,1:2],siatka[i,],l)
  }
}
nowa_ramka3$Class=dane[,3]
glmnet3 <- glmnet(x=nowa_ramka3[,1:6],y=nowa_ramka3$Class,family="multinomial",
               lambda=0.001,alpha=0)
wynik3 <- train(Class~.,data=nowa_ramka3,method="glmnet",
      trControl=trainControl("cv"))
wynik3a <- train(Class~.,data=nowa_ramka3,method="glmnet",
      trControl=trainControl("LOOCV"))
wynik3
```


```{r, echo=FALSE, warning=FALSE, message=FALSE}
set.seed(456)
nowa_ramka4 = data.frame()
phi2=function(punkt,sr,os){
  exp(-sum((punkt-sr)^2)/(2*os^2))
}
n5=4
n6=1
l2=3
mu5=seq(-7,7,length.out = n5)
mu6=seq(-1,7,length.out = n6)
siatka=expand.grid(mu5=mu5,mu6=mu6)
for(i in 1:(n5*n6)){
  for(j in 1:266){
    nowa_ramka4[j,i]=phi2(dane[j,1:2],siatka[i,],l2)
  }
}
nowa_ramka4$Class=dane[,3]
glmnet4 <- glmnet(x=nowa_ramka4[,1:4],y=nowa_ramka4$Class,family="multinomial",
               lambda=0.001,alpha=0)
wynik4 <- train(Class~.,data=nowa_ramka4,method="glmnet",
      trControl=trainControl("cv"))
wyni4a <- train(Class~.,data=nowa_ramka4,method="glmnet",
      trControl=trainControl("LOOCV"))
wynik4
```

<font size = 4>For both base functions, the level of correct classification was high. The greatest difficulty for the classifier were observations from class 3, which it was unable to separate
from the rest and only for them a classification error occurred.</font>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
plot(dane$A,dane$B,col = predict.train(wynik4))
```

<font size = 4>Changes in the l and m_i coefficients only slightly changed the classification areas.</font>
